<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Tales of a Postgraduate Nothing]]></title><description><![CDATA[/home/tillson/*]]></description><link>https://tillsongalloway.com/</link><image><url>https://tillsongalloway.com/favicon.png</url><title>/home/tillson/*</title><link>https://tillsongalloway.com/</link></image><generator>Ghost 3.17</generator><lastBuildDate>Tue, 26 May 2020 21:28:44 GMT</lastBuildDate><atom:link href="https://tillsongalloway.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[How I made $10K in bug bounties from GitHub secret leaks]]></title><description><![CDATA[Hacker can steal leaked API keys, passwords, and customer data from GitHub to login to servers, steal personal information, and rack up absurd AWS charges. This article explains shows common types of secrets that users post and how to find them.]]></description><link>https://tillsongalloway.com/finding-sensitive-information-on-github/</link><guid isPermaLink="false">5eb84c8afb4b1ab8bb3baa3d</guid><category><![CDATA[cybersecurity]]></category><category><![CDATA[hacking]]></category><category><![CDATA[osint]]></category><category><![CDATA[github]]></category><category><![CDATA[bug bounties]]></category><category><![CDATA[technical]]></category><dc:creator><![CDATA[Tillson Galloway]]></dc:creator><pubDate>Sun, 10 May 2020 21:05:56 GMT</pubDate><media:content url="https://tillsongalloway.com/content/images/2020/05/Screen-Shot-2020-05-11-at-1.05.48-AM.png" medium="image"/><content:encoded><![CDATA[<img src="https://tillsongalloway.com/content/images/2020/05/Screen-Shot-2020-05-11-at-1.05.48-AM.png" alt="How I made $10K in bug bounties from GitHub secret leaks"><p>API keys, passwords, and customer data are accidentally posted to GitHub every day. </p><p>Hackers use these keys to login to servers, steal personal information, and rack up absurd AWS charges. GitHub leaks can cost a company thousands–or even millions–of dollars in damages. Open-source intelligence gathering on GitHub has become a powerful arrow in every security researcher's quiver: researchers from NC State even wrote an <a href="https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_04B-3_Meli_paper.pdf">academic paper</a> on the subject. </p><p>This article, written for both bug bounty hunters and enterprise infosec teams, demonstrates common types of sensitive information (secrets) that users post to public GitHub repositories as well as heuristics for finding them. The techniques in this article can be applied to <a href="https://gist.github.com/">GitHub Gist</a> snippets, too.</p><p>In the last year, I've earned nearly $10,000 from bug bounty programs on <a href="https://hackerone.com">HackerOne</a> without even visiting programs' websites thanks to these techniques. I've submitted over 30 Coordinated Disclosure reports to vulnerable corporations, including eight Fortune 500 companies. </p><p><strong>I've also released <a href="https://github.com/tillson/git-hound">GitHound</a>, an open-source tool designed to automate the process of finding keys across GitHub.</strong> GitHound isn't limited to a single user or organization: it sifts through all of GitHub, using Code Search queries as an entrypoint into repositories and then using context, regexes, and some other neat tricks to find secrets.</p><h2 id="github-code-search">GitHub Code Search</h2><p>Before we get into the automated tools and bug bounty strategies, let's talk about Code Search. </p><p>GitHub provides <a href="https://github.com/search">rich code searching</a> that scans public GitHub repositories (some content is omitted, <a href="https://help.github.com/en/github/searching-for-information-on-github/searching-code#considerations-for-code-search">like forks and non-default branches</a>). Queries can be simple like <code>uberinternal.com</code> or can contain multi-word strings like  <code>"Authorization: Bearer"</code>. Searches can even target specific files (<code>filename: vim_settings.xml</code>) or specific languages (<code>language:SQL</code>). Searches can also contain certain <a href="https://help.github.com/en/github/searching-for-information-on-github/understanding-the-search-syntax">boolean qualifiers</a> like <code>NOT</code> and <code>&gt;</code>. </p><p>Knowing the rules of GitHub code search enables us to craft search dorks: queries that are designed to find sensitive information. GitHub dorks can be found online, but the best dorks are the ones that you create yourself.</p><p>For example, <code>filename: vim_settings.xml</code> (<a href="https://github.com/search?q=filename%3Avim_settings.xml&amp;type=Code">try it!</a>) targets <a href="https://www.jetbrains.com/help/idea/settings-tools-settings-repository.html">IntelliJ settings files</a><a>.</a> Interestingly, the <code>vim_settings.xml</code> file contains recent <strong>copy-pasted strings encoded in Base64</strong>. I recently made $2400 from a bug bounty with this dork: SaaS API keys and customer information were exposed in <code>vim_settings.xml</code>.</p><figure class="kg-card kg-image-card"><img src="https://tillsongalloway.com/content/images/2020/05/vim_settings.png" class="kg-image" alt="How I made $10K in bug bounties from GitHub secret leaks"></figure><p><code>vim_settings.xml</code> only contains recently copy-pasted strings, but we can exploit the repository's commit history to find the <strong>entire copy-paste history.</strong> Just clone the repository and run <a href="https://gist.github.com/tillson/620e8ef87bc057f25b0a27c423433fda">this 14-line script</a>, and the user's activity will be at your fingertips. GitHound also finds and scans base64 encoded strings for secrets, even in commit history.</p><p>By the way: with <a href="https://github.com/search?q=%22vim_settings.xml%22&amp;type=Commits">a GitHub commit search dork</a>, we can quickly scan all 500,000 of commits that edit <code>vim_settings.xml</code>.</p><figure class="kg-card kg-image-card"><img src="https://tillsongalloway.com/content/images/2020/05/commits.png" class="kg-image" alt="How I made $10K in bug bounties from GitHub secret leaks"></figure><h2 id="search-heuristics-for-bug-bounty-hunters">Search Heuristics for Bug Bounty Hunters</h2><p>GitHub dorks broadly find sensitive information, but<strong> what if we want to look for information about a specific company?</strong> GitHub has millions of repositories and even more files, so we'll need some heuristics to narrow down the search space. </p><p>To start finding sensitive information, identify a target. </p><p>I've found that the best way to start is to <strong>find domains or subdomains that identify corporate infrastructure.</strong> </p><p>Searching for <code>company.com</code> probably won't provide useful results: many companies release audited open-source projects that aren't likely to contain secrets. Less-used domains and subdomains are more interesting. This includes specific hosts like <code>jira.company.com</code> as well as more general second-level and lower-level domains. It's more efficient to find a pattern than a single domain: <code>corp.somecompany.com</code>, <code>somecompany.net</code>, or <code>companycorp.com</code> are more likely to appear only in an employee's configuration files. </p><p>The usual suspects for open-source intelligence and domain reconnaissance help here:</p><ul><li><a href="https://github.com/TheRook/subbrute">Subbrute</a> - Python tool for brute-forcing subdomains</li><li><a href="https://www.threatcrowd.org/">ThreatCrowd</a> - Given a domain, find associated domains through multiple OSINT techniques</li><li><a href="https://censys.io/">Censys.io</a> - Given a domain, find SSL certificates using it</li></ul><p>GitHound can help with subdomain discovery too: add a custom regex <code>\.company\.com</code> and run GitHound with the <code>--regex-file</code> flag.</p><p>After finding a host or pattern to search, play around on GitHub search with it (I always do this before using automated tools). There are a few questions I like to ask myself here:</p><ol><li><strong>How many results came up?</strong> If there are over 100 pages, I'll likely need to find a better query to start with (GitHub limits code search results to 100 pages).</li><li><strong>What kind of results came up?</strong> If the results are mostly (intentionally) open-source projects and people using public APIs, then I may be able to refine the search to eliminate those.</li><li><strong>What happens if I change the language?</strong> <code>language:Shell</code> and <code>language:SQL</code> may have interesting results.</li><li><strong>Do these results reveal any other domains or hosts?</strong> Results in the first few pages will often include a reference to another domain (e.g. searching for <code>jira.uber.com</code> may reveal the existence of another domain entirely, like <code>uberinternal.com</code>).</li></ol><p>I spend most of my time in this step.</p><p>It's crucial that the search space is well-defined and accurate. Automated tools and manual searching will be faster and more accurate with the proper query.</p><p>Once I find results that seem interesting based on the criteria above, I run it through <a href="https://github.com/tillson/git-hound">GitHound</a><a> </a>with <code>--dig-files</code> and <code>--dig-commits</code> to look the entire repository and its history. </p><p><code>echo "uberinternal.com" | ./git-hound --dig-files --dig-commits</code></p><p><code>echo "uber.com" | ./git-hound --dig-files --language-file languages.txt --dig-commits</code></p><p><code>echo "uber.box.net" | ./git-hound --dig-files --dig-commits</code></p><p>GitHound also locates interesting files that simply searching won't find, like <code>.zip</code> or <code>.xlsx</code> files. Importantly, I also manually go through results since automated tools often miss customer information, sensitive code, and username/password combinations. Oftentimes, this will reveal more subdomains or other interesting patterns that will give me ideas for more search queries. It's important to remember that open-source intelligence is a recursive process.</p><p>This process almost always finds results. Leaks usually fall into one of these categories (ranked from most to least impactful):</p><ol><li><strong>SaaS API keys</strong> - Companies rarely impose IP restrictions on APIs. AWS, Slack, Google, and other API keys are liquid gold. These are usually found in config files, bash history files, and scripts.</li><li><strong>Server/database credentials</strong> - These are usually behind a firewall, so they're less impactful. Usually found in config files, bash history files, and scripts.</li><li><strong>Customer/employee information</strong> - These hide in XLSX, CSV, and XML files and range from emails all the way to billing information and employee performance reviews.</li><li><strong>Data science scripts</strong> - SQL queries, R scripts, and Jupyter projects can reveal sensitive information. These repos also tend to have "test data" files hanging around.</li><li><strong>Hostnames/metadata</strong> - The most common result. Most companies don't consider this a vulnerability, but they can help refine future searches</li></ol><h2 id="workflow-for-specific-api-providers">Workflow for Specific API Providers</h2><p>Dorks can also be created to target specific API providers and their endpoints. This is especially useful for companies creating automated checks for their users' API keys. With knowledge of an API key's <strong>context</strong> and <strong>syntax</strong>, the search space can be significantly reduced. </p><p>With knowledge of the specific API provider, we can obtain all of the keys that match the API provider's regex and are in an API call context and then we can check them for validity using an internal database or an API endpoint. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://tillsongalloway.com/content/images/2020/05/graph.png" class="kg-image" alt="How I made $10K in bug bounties from GitHub secret leaks"><figcaption>A workflow for finding secrets for a single API provider</figcaption></figure><p>For example, suppose a company (HalCorp) provides an API for users to read and write to their account. By making our own HalCorp account, we discover that API keys are in the form <code>[a-f]{4}-[a-f]{4}-[a-f]{4}</code>. </p><pre><code># Python
import halapi
api = halapi.API()
api.authenticate_by_key('REDACTED')

# REST API with curl
curl -X POST -H "HALCorp-Key: REDACTED" https://api.halcorp.biz/userinfo
</code></pre><p>Armed with this information, we can compose our own GitHub dorks for HalCorp API responses: </p><pre><code># Python
"authenticate_by_key" "halapi" language:python

# REST API
"HALCorp-Key"
</code></pre><p>With a tool like <a href="https://github.com/tillson/git-hound">GitHound</a><a>,</a> we can use regex matching to find strings that match the API key's regex and output them to a file:</p><p><code>echo "HALCorp-Key" | git-hound --dig-files --dig-commits --many-results --regex-file halcorp-api-keys.txt --results-only &gt; api_tokens.txt </code></p><p>Now that we have a file containing potential  API tokens, and we can check these against a database for validity (<strong>do not do this if you don't have written permission from the API provider</strong>).</p><p>In the case of HalCorp, we can write a bash script that reads from stdin, checks the <code>api.halcorp.biz/userinfo</code> endpoint, and outputs the response.</p><p><code>cat api_tokens.txt | bash checktoken.bash</code></p><h2 id="remediation">Remediation</h2><p>Although awareness of secret exposure on GitHub has increased, more and more sensitive data are published each day. </p><p>Amazon Web Services have begun <a href="https://aws.amazon.com/blogs/security/how-to-receive-notifications-when-your-aws-accounts-root-access-keys-are-used/">notifying users if their API keys are posted online</a>. GitHub has added <a href="https://github.com/features/security">security features</a> that scan public repositories for common keys. These solutions are merely bandaids, however. To limit secret leaks from source code, we must update API frameworks and DevOps methodologies to prevent API keys from being stored in Git/SVN repositories entirely. Software like <a href="https://www.vaultproject.io/">Vault</a> safely stores production keys and some API providers, like Google Cloud Platform, have updated their libraries to force API keys to be stored in a file by default.</p><p>Fully eradicating exposure of sensitive information is a more difficult problem: how can customer information be fully detected? What if it's in a Word, Excel, or compiled file? More research must be conducted in this field to study the extent of the problem and its solution.</p>]]></content:encoded></item><item><title><![CDATA[Nothing Lasts, but Nothing is Lost: Taoist Ideology in Slaughterhouse Five’s Tralfamadorians]]></title><description><![CDATA[The Tralfamadorian worldview of Slaughterhouse Five fame surrenders to time's unrelenting waters, its central axiom being that “the moment simply is."  Tralfamadorian physics lead us to question our perceptions of time.]]></description><link>https://tillsongalloway.com/taoism-in-slaughterhouse-five/</link><guid isPermaLink="false">5c158ee25601a73d67242733</guid><category><![CDATA[philosophy]]></category><category><![CDATA[literature]]></category><category><![CDATA[taoism]]></category><dc:creator><![CDATA[Tillson Galloway]]></dc:creator><pubDate>Sat, 15 Dec 2018 23:33:43 GMT</pubDate><content:encoded><![CDATA[<blockquote><em>“The wise does not speak.  He who speaks is not wise." – Lao Tzu</em></blockquote><p>The Tralfamadorian worldview of <em>Slaughterhouse Five</em> fame surrenders to time's unrelenting waters, its central axiom being that “the moment simply is.” <sup><a href="#f1">[1]</a></sup> Tralfamadorian physics lead us to question our perceptions of time.  If past, present, and future all happen in one moment, can anything start or stop existing?  If we die, what happens to our consciousness?  Is essence conserved like energy?</p><p>Lao Tzu describes the Tao as “complete and perfect as a wholeness” existing “everywhere and anywhere” as the “eternal law.” <sup><a href="#2">[2]</a></sup>  The Tao exists as one indivisible entity and flows like water: it “benefits all things and contends not with them."  Water, vital for life, is uncaring of anything that stands in its way.  The water in Taoism symbolizes the eternal "oneness" that encapsulates space and time.  In any competition held with respect to time, water wins in the end.  Following the Tao symbolizes embrace and submission to the water; it represents the art of doing without doing.  It represents just living your life.  </p><p>Tralfamadorians accept the nature of the Tao, the notion that time is one entity, as opposed to the supposed human “illusion that one moment follows another one like beads on a string, and that once a moment is gone it is gone forever.” <sup><a href="#f1">[1]</a></sup>  For Tralfamadorians, the Tao is embodied by the concept of a moment.  We see this further exemplified when the Tralfamadorians explain death to Billy.  “When a Tralfamadorian sees a corpse,” they explain, “all he thinks is that the dead person is in a bad condition in that particular moment, but that the same person is just fine in plenty of other moments.” <sup><a href="#f3">[3]</a></sup>  Tralfamadorian thanatology, it appears, aligns with the Tao.  The contrast between the Tralfamadorian metaphor for time, “a bug trapped in amber” <sup><a href="#f1">[1]</a></sup> and the metaphor of water present throughout the <em>Tao Te Ching </em>presents a powerful juxtaposition. Vonnegut’s Tralfamadorians possess a cynical but rational view of time, portraying the universe as "trapped" within its amber, while Taoists paradoxically experience it as an eternal flow with divine beauty.  Both agree, definitively, that time is inescapable and is unconcerned with the affairs of Creation.</p><p>Billy Pilgrim, the “unstuck in time” subject of Slaughterhouse Five, exists in his own purgatory, crossed between Newtonian and Tralfamadorian spacetime.  He has lived his life time and time again, living memories <em>ad hoc</em>.  He remains stoic throughout <em>Five</em>, knowing the outcomes of any situation up to and including his death, so it goes. But Billy understands the Moment, the Tao, better than anyone else.  Whether he is a WWII prisoner of war or an exhibit in a Tralfamadorian zoo, Billy does not try to change the Moment: he lives it.</p><hr><!--kg-card-begin: html--><div style="text-align:left"><div id="f1"><!--kg-card-end: html--><p>[1] Chapter 4, Vonnegut's Slaughterhouse Five.</p><!--kg-card-begin: html--></div><div id="f2"><!--kg-card-end: html--><p><sup>[2]</sup> Chapters 2, 4, and 16, respectively, of Lao Tzu's <em>Tao Te Ching.</em></p><!--kg-card-begin: html--></div><div id="f3"><!--kg-card-end: html--><p><sup>[3]</sup> Chapter 2, Vonnegut's <em>Slaughterhouse Five.</em></p><!--kg-card-begin: html--></div></div><!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[Running an Introductory Level CTF: Insight into Porter-Gaud CTF 2017]]></title><description><![CDATA[On 28 January, we (Charles Truluck, Cameron Hay, and myself) ran the inaugural Porter-Gaud CTF competition, an introductory cyber-security red team competition designed to give high schoolers from around South Carolina a hands-on experience with security concepts.]]></description><link>https://tillsongalloway.com/porter-gaud-ctf-2017/</link><guid isPermaLink="false">5c158a455601a73d67242721</guid><category><![CDATA[cybersecurity]]></category><category><![CDATA[writeup]]></category><category><![CDATA[technical]]></category><category><![CDATA[CTF]]></category><category><![CDATA[hacking]]></category><category><![CDATA[events]]></category><dc:creator><![CDATA[Tillson Galloway]]></dc:creator><pubDate>Sat, 15 Dec 2018 23:12:19 GMT</pubDate><media:content url="https://tillsongalloway.com/content/images/2020/05/PGCTF.png" medium="image"/><content:encoded><![CDATA[<img src="https://tillsongalloway.com/content/images/2020/05/PGCTF.png" alt="Running an Introductory Level CTF: Insight into Porter-Gaud CTF 2017"><p>On 28 January, we (Charles Truluck, Cameron Hay, and myself) ran the inaugural Porter-Gaud CTF competition, an introductory cyber-security red team competition designed to give high schoolers from around South Carolina a hands-on experience with security concepts.</p><h4 id="inception">Inception</h4><p>After competing in the <a href="https://nodesc.org/">NodeSC</a> 2016 CTF and <a href="http://pcdc-sc.com/">PCDC</a> here in Charleston, we immediately knew that we wanted to run our own capture-the-flag competition. We went to Doug Bergman, the computer science chair at Porter-Gaud, and began to make the idea a reality.</p><h4 id="designing-the-problems">Designing the Problems</h4><p>My personal Computer Science III (a semester class at Porter-Gaud) was to design and create the competition, which proved to be time-consuming.</p><figure class="kg-card kg-image-card"><img src="https://tillsongalloway.com/content/images/2018/12/IMG_0739.jpg" class="kg-image" alt="Running an Introductory Level CTF: Insight into Porter-Gaud CTF 2017"></figure><p>In this picture, you can see the Jeopardy board that lived in the Porter-Gaud computer lab from September-January with all of the problems from the competition, along with a few that did not make it to the competition for various reasons (difficulty, time constraints, too many problems)</p><p>After developing a problem, we would test it on each other for difficulty and usability.</p><h4 id="reaching-out">Reaching Out</h4><p>The hardest part of organizing PGCTF was reaching out and getting teams to sign up. <br>We officially announced the competition in early-November, but we did not have enough teams to successfully run the competition until early January. Teams continued rolling in up until four days before the competition. <br>Next year, we're planning to improve our outreach.</p><h4 id="tech-week">Tech Week</h4><p><em>(warning: technical section)</em></p><p>The final step in putting together the CTF was setting up the infrastructure. <br>While we initially planned to have multiple servers (scoreboard, web problems, file server), we abandoned this idea due to a lack of resources and used one server running Ubuntu.</p><h6 id="quality-assurance">Quality Assurance</h6><p>During the pre-competition week, we made it a goal to work through all of the problems to ensure that they were solvable, the anticipated difficulty, and fun to solve. <br>This led us to change point values of a few problems (ADFGX, for example).</p><h6 id="problem-servers">Problem Servers</h6><p>We used <a href="https://www.docker.com/">Docker</a> to handle having multiple interfaces and therefore multiple IP addresses leading to different websites. Docker also allowed for us to isolate problems for each other due to the exploitative nature of the competition (for example, <a href="https://github.com/tillson/pgctf-problems/tree/master/web/I-cant-c">Imgur 2.0</a> was solvable by obtaining a shell on the host server) <br>For the <a href="https://github.com/tillson/pgctf-problems/tree/master/web">web problems</a> and <a href="https://github.com/tillson/pgctf-problems/tree/master/algorithm/algo2">Algorithm 1</a>, we had to host webservers. We didn't want to bloat the ports on one host, so we used a trick with Docker and interfaces to create an illusion of there being multiple servers.</p><p>In order to create these interfaces, we used <a href="https://jpetazzo.github.io/2013/10/16/configure-docker-bridge-network/">this</a> guide, which led to:</p><pre><code>$ ip link add icantc type bridge
$ ip addr add 10.0.0.21 dev icantc
$ ip link set icantc up
</code></pre><p>Becuase of an issue with the <code>/etc/network/interfaces</code> file (most likely caused by syntax issues) discovered the night before the competition, we wrote a script that we would run whenever the master server came up.</p><h6 id="dockerfiles">Dockerfiles</h6><p>In order to set up the problem servers, we had to set up Dockerfiles for each of the web-based problems. While <a href="https://github.com/tillson/pgctf-problems/blob/master/web/misc-web-server/Dockerfile">Apache/PHP Dockerfiles</a> were pretty easy, we were unable to fully setup NodeJS ones. We wound up having to manually start the NodeJS instance within the Docker containers on competition day.</p><h6 id="wifi-or-ethernet">Wifi or Ethernet?</h6><p>During the pre-competition week, we had to build the full network. The biggest disagreement between the PGCTF Team was whether to use WiFi or to stick to Ethernet. While Wifi would be more convenient, it was more prone to breaking during the course of the competition. Ethernet would be more stable, but users with newer computers may require an adapter.We eventually settled on using Wifi with a Switch in the room if a team opted to use Ethernet.</p><h6 id="the-nightmare-of-dhcp">The Nightmare of DHCP</h6><p>One problem that came up after creating these interfaces was that computers connecting to the access point would be assigned the same IP as one of our defined interfaces on the server, and would not be able to connect to anything (and would break the problem). We solved this by modifying the DHCP scope to exclude the 10.1.10.20-30 IP range, which was dedicated to problems.</p><figure class="kg-card kg-image-card"><img src="http://tillsongalloway.com/blog/content/images/2017/02/Untitled.png" class="kg-image" alt="Running an Introductory Level CTF: Insight into Porter-Gaud CTF 2017"></figure><p>Here's an MS-Paint rendition of the network diagram.</p><figure class="kg-card kg-image-card"><img src="https://tillsongalloway.com/content/images/2018/12/Untitled.png" class="kg-image" alt="Running an Introductory Level CTF: Insight into Porter-Gaud CTF 2017"></figure><h6 id="documents">Documents</h6><p>On Friday night, we finalized the competition scope and printed the documents that were distributed to each team with rules/schedule/scope of the competition. We also finalized the intro powerpoint and the itinerary for the day.</p><h4 id="competition-day">Competition Day</h4><p>Arriving at Porter-Gaud around 8:45 for the 9:00 start, I was impressed to see that teams had already begun arriving and were setting up their team spaces. <br>As I walked in and noticed the list of IP addresses both printed on each team's table and on the whiteboard, I realized that the web servers were all up and could be exploited before the intended start time. Luckily, I realized this before the teams did and shut them down. <br>At 9:15, about fifteen minutes before the originally planned start time, all teams were ready to go and we began the competition.</p><p>Though there were a few mishaps throughout the day (a file missing from a problem, for example), everything ran <em>extremely</em> smoothly. <br>Teams solved the <a href="https://github.com/tillson/pgctf-problems/tree/master/misc/recon/Tillson">Tillson Galloway recon</a> problem much faster than I expected them to, and it was very rewarding watching teams finally solve the last step of the problem. <br><a href="https://github.com/tillson/pgctf-problems/tree/master/forensics/the-d-in-detroit">The D in Detroit</a> was also a favorite of both organizers and participants, and all teams eventually solved it (though the room was filled with static noises for the first two hours).</p><p>While I expected the competition to die down around 1:30 as in past CTFs I've competed in, teams stayed interested and kept the fight for first/second place intact until the last minute.</p><h4 id="after-the-competition">After the Competition</h4><h6 id="survey">Survey</h6><p>A few days after the competition, I sent out an email with resources to future CTFs and an anonymous survey for PGCTF. The survey truly will help us outline next year's competition as it was the most direct way to give feedback.</p><p>A couple of stats from the survey: <br>Favorite problems were: <br>* <a href="https://github.com/tillson/pgctf-problems/tree/master/crypto/ADFGX">ADFGX (Crypto)</a> <br>* <a href="https://github.com/tillson/pgctf-problems/tree/master/misc/recon/Tillson">Tillson Galloway (Recon/Misc)</a> <br>* <a href="https://github.com/tillson/pgctf-problems/tree/master/forensics/the-d-in-detroit">The D in Detroit (Forensics)</a></p><h4 id="afterthoughts">Afterthoughts</h4><h6 id="lessons-learned">Lessons Learned</h6><p>Proofreading and testing the problems before the competition is essential. There were two instances throughout the day where a web problem link was either wrong or the problem was missing entirely.</p><p>We also found that one area we can improve in is evaluating problem difficulty. There were instances where problems that were meant to be easy ended up being much harder than anticipated, which steered teams away from the category at large (<a href="https://github.com/tillson/pgctf-problems/tree/master/web/ecorp-internal">E-Corp Internal</a> and <a href="https://github.com/tillson/pgctf-problems/tree/master/Archlinux%20VM">ArchLinux</a>, for example). We'll try to provide better resources for training next year, and we may release challenge problems throughout the year.</p><h6 id="next-year">Next Year</h6><p>Next year, we also want to have a more interactive experience the day of the game, expanding upon the MS08-067 Windows XP image released mid-game in order to keep the game lively.</p><h4 id="what-s-next">What's Next?</h4><p>The Dangling Pointers will be competing in EasyCTF next week as well as The Palmetto Cyber Defense Competition on 8 April.</p><h4 id="thank-you">Thank You</h4><p>I'd like to close this writeup with a thank you to the following people for their help with the event:</p><ul><li><strong><strong>Charles Truluck</strong></strong>, for both helping build and setup the server as well as for building and testing problems</li><li><strong><strong>Cameron Hay</strong></strong>, for work on the cryptography category and support for teams on the day of the event.</li><li><strong><strong>Bryan Luce and Doug Bergman</strong></strong>, for all around support and motivation for running the event, advising us during the production of it, and handling communication with the school administration in order to legitimize the event.</li><li><strong><strong>Phil Zaubi</strong></strong>, for setting up the final networking component of the event.</li></ul><p></p><p><em>Originally published 2 March 2017</em></p>]]></content:encoded></item></channel></rss>